{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 5 -  Newton's method: logistic regression\n",
    "\n",
    "\n",
    "<h4 align=\"right\"> Hicham Janati </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let $f$ be a function in $\\mathcal{C}^1(\\mathbb{R}^n, \\mathbb{R}^n)$. The goal of Newton's method is to solve the equation $f(x) = 0$ using a first order approximation.\n",
    "\n",
    "Let $x_0 \\in \\mathbb{R}^n$. Consider the first order approximation of f at $x_0$. Solving $f(x) = 0$ by cancelling its first order approximation leads to:\n",
    "$ f(x_0) + J_f(x_0)(x - x_0) = 0.$ Thus:\n",
    "$$ x_1 = x_0 - J_f(x_0)^{-1}(f(x_0)) $$\n",
    "Iterating this procedure by a cancelling the approximation of $f$ at $x_1$ leads to the Newton sequence:\n",
    "$$ x_{k+1} = x_k - J_f(x_k)^{-1}(f(x_k)) $$\n",
    "\n",
    "#### Theorem: local convergence\n",
    "Let $x^{\\star}$ be a zero de $f$ and $x_k$ is Newton's sequence. Assume that $J(x^{\\star})$ is invertible and that $f$ is in $\\mathcal{C}^2$. Newton's method converges quadratically locally i.e there exists a ball centered around $x^{\\star}$ such that for all $x_0$ in B, there exists $\\alpha > 0$ such that : $$ \\|x_{k+1} - x^{\\star}\\| < \\alpha \\|x_{k} - x^{\\star}\\|^2 $$\n",
    "\n",
    "Let $g$ be a function in $\\mathcal{C}^2$ from $\\mathbb{R}^n$ to $\\mathbb{R}$.\n",
    "\n",
    "## I - Minimizing quadratic functions\n",
    "\n",
    "##### Question 1\n",
    "Adapt Newton's method to solve a strictly convex problem $\\min_{x \\in \\mathbb{R}^n} g(x)$.\n",
    "\n",
    "##### Question 2\n",
    "Consider a symmetric positive definite matrix $A \\in \\mathbb{R}^{n, n}$, $b\\in\\mathbb{R}^n$ and define the quadratic function:\n",
    "$$ f(x) = \\frac{1}{2}x^\\top A x + b^\\top x + c $$\n",
    "Compute the gradient and hessian of $f$. Deduce the solution of \n",
    "$$ \\min_{x \\in \\mathbb{R}^n} f(x) $$\n",
    "\n",
    "##### Question 3\n",
    "Apply Newton's method to the the problem above. How many iterations does Newton's method need to converge ?\n",
    "\n",
    "\n",
    "#####  Question 4\n",
    "Consider a smooth differentiable function $f$. For what function $\\hat{f_k}$ Newton's sequence to solve $\\min_x f(x)$ is equivalent to:\n",
    "$$ x_{k+1} = \\min_{x} \\hat{f_k}(x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## II -  Logistic regression\n",
    "Consider a binary classification problem where given a sample $X_i \\in \\mathbb{R}^d$, we would like to predict a label $y_i \\in \\{-1, 1\\}$. To predict $y$, we can learn a function $f$ and use its sign to get a value in $\\{-1, 1\\}$. Taking a simple linear model $f(x) = w^\\top x$ leads to the problem:\n",
    "\n",
    "$$ \\min_{w \\in \\mathbb{R}^d} \\sum_{i=1}^n |1 - y_i \\text{sign}(w^\\top x_i)|^2 $$\n",
    "\n",
    "This loss will be minimized when $f(x_i)$ and $y_i$ have the same sign. The sign function is however not differentiable (not even continuous) and thus hard to minimize. Instead, we model the conditional probability using the -- differentiable -- sigmoid function $\\sigma$:\n",
    "\n",
    "$$ P(Y = y | w) = \\sigma(y w^\\top x) = \\frac{1}{1 + \\exp(- y w^\\top x))} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some classification data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "rng = np.random.RandomState(42)\n",
    "n_samples, dim = 200, 3\n",
    "X, y = make_classification(n_samples=n_samples,\n",
    "    n_features=dim, n_redundant=0, n_informative=2, random_state=rng, n_clusters_per_class=1\n",
    ")\n",
    "y[y == 0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1\n",
    "\n",
    "Verify that this probabilistic model is well defined and compute its negative log-likelihood for i.i.d samples $(X_1, y_1)\\dots (X_n, y_n)$. \n",
    "\n",
    "\n",
    "##### Question 2\n",
    "What is the maximum-a-posteriori estimate of $w$ if we assume that the prior on $w$ is a Gaussian $\\mathcal{N}(0, C)$ with $C > 0$ ?\n",
    "\n",
    "##### Question 3\n",
    "Consider the regularized logistic regression problem:\n",
    " \n",
    " $$g: w \\mapsto \\sum_{i=1}^n \\log(1 + exp(- y_i w^\\top X_i) + \\frac{1}{2C} \\|w\\|^2.$$\n",
    "\n",
    "The learned classifier is then given by $f(x) = sign(w^\\top x)$. \n",
    "\n",
    "The sigmoid function is defined as $\\sigma: u \\to \\frac{1}{1 + \\exp(-u)}$ and the logisitc loss function $\\ell(u) = -\\log(\\sigma(u)) = \\log(1 + \\exp(-u)).$\n",
    "\n",
    "Show the following:\n",
    "1. $\\sigma(-u) = 1 - \\sigma(u)$\n",
    "2. $\\sigma'(u) = \\sigma(u)\\sigma(-u)$\n",
    "3. $\\ell'(u) = \\sigma(u) - 1$\n",
    "\n",
    "##### Question 4\n",
    "\n",
    "Show that: \n",
    "$\\nabla g(w) =  - \\sum_{i=1}^n y_i \\sigma(-y_i w^\\top X_i) X_i + \\frac{1}{C} w$ and complete the gradient function below.\n",
    "\n",
    "#### Question 5:\n",
    "Show that\n",
    "$\\nabla^2g(w) = \\sum_{i=1}^n \\sigma(y_i w^\\top X_i)(1 - \\sigma(y_i w^\\top X_i))X_i X_i^\\top + \\frac{1}{C} I_p$\n",
    "\n",
    "Use ```scipy.optimize.check_grad``` to verify numerically the gradient function on some chosen points. Adapt the functions to check the hessian as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(u):\n",
    "    pass\n",
    "\n",
    "def logistic(u):\n",
    "    pass\n",
    "\n",
    "def g(w, C=1.):\n",
    "    pass\n",
    "\n",
    "def gradient_g(w, C=1.):\n",
    "    pass\n",
    "\n",
    "def hessian_g_(w, C=1.):\n",
    "    pass\n",
    "\n",
    "def hessian_g(w, C=1.):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 6\n",
    "Complete the function `newton` below to solve the problem. Computing the inverse of the hessian has a  $O(n^3)$ complexity, how can we write the Newton iteration without inverting any matrix? Try the newton algorithm and verify that it converges with a random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def newton(w0, g=g, gradient=gradient_g, hessian=hessian_g, C=1., maxiter=10, verbose=True):\n",
    "    \"\"\"Solve min g with newton method\"\"\"\n",
    "    w = w0\n",
    "    if verbose:\n",
    "        strings = [\"Iteration\", \"g(w_k)\", \"max|gradient(w_k)|\"]\n",
    "        strings = [s.center(13) for s in strings]\n",
    "        strings = \" | \".join(strings)\n",
    "        print(strings)\n",
    "\n",
    "    for i in range(maxiter):\n",
    "\n",
    "        if verbose:\n",
    "            obj = g(w, C=C)\n",
    "            strings = [i, obj, abs(d).max()] # On affiche des trucs \n",
    "            strings = [str(s).center(13) for s in strings]\n",
    "            strings = \" | \".join(strings)\n",
    "            print(strings)\n",
    "        \n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = rng.randn(dim)\n",
    "w = newton(w0, C=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "In machine learning, instead we rather use quasi-Newton methods (Powell, BFGS, LBFGS) where the inverse of the hessian is approximated along the iterations which are implemented in Scipy. \n",
    "Using scipy's implementation, verify that your obtained solution is identical to the one given by lbfgs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 8\n",
    "Compare w witht the obtained with scikit-learn's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
